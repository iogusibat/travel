---
title: HadoopのデータをSQLで処理する「SQL on Hadoop」の可能性（前編）―日本ユニシス | IT Leaders
updated: 2016-12-07 09:04:25Z
created: 2016-12-07 09:04:25Z
source: http://it.impressbm.co.jp/articles/-/14096
tags:
  - SI
---

# HadoopのデータをSQLで処理する「SQL on Hadoop」の可能性（前編）―日本ユニシス

清酒文人 小原未生（日本ユニシス）

2010年代に入ってから、ビッグデータが多くの企業から注目されている。その理由は、ビッグデータを活用することで、同業他社との差別化を図り、売上拡大や業務効率化により企業価値を向上させることができるからである。ビッグデータを蓄積/処理する基盤としてDWH向けRDBMSやHadoopがあり、成り立ちやアーキテクチャは異なるが、データを格納して、SQLをインタフェースとしてデータにアクセスすることができる点など、利用者から見ると違いが分かり難くなってきている。そこで日本ユニシスでは、DWH向けRDBMSとHadoopを用いて、データロード、データ検索について性能面での比較検証を行い、SQL on Hadoop（Hadoop内のデータをSQLで処理する機能）の適用範囲について考察を行った。本稿では検証結果と考察を報告する・※本稿は日本ユニシス発行の「技報通巻127号」（2016年3月発行）の記事に加筆・編集して掲載しています。

　2011年に米国の調査会社McKinsey & Companyの研究部門であるMcKinsey Global Instituteが発表したレポート「Big data: The next frontier for innovation, competition, and productivity」によりビッグデータが注目を集めており、それ以降、本格的にビッグデータの活用に取り組む企業が増えてきている。その理由は、ビッグデータを活用することで同業他社との差別化を図り、売上拡大や業務効率化により企業価値を向上させることができるからである。

　スマートフォンやSNS（Social Networking Service）などの普及により、膨大かつ多様なデータが短時間に発生している。2020年には約40ZB（ゼタバイト）のデータが発生すると言われている。これらのデータを蓄積する基盤としては、データウェアハウス（以下、DWH）専用のリレーショナルデータベース管理システム（以下、RDBMS）とHadoopがある。両方とも成り立ちやアーキテクチャは異なるが、データを格納して、SQLをインタフェースとしてデータにアクセスすることができる点など、利用者から見ると違いが分かり難くなってきている。

## ビッグデータが注目されている背景と課題

### ビッグデータが注目されている背景

　エンタープライズシステムにおけるデータ活用は、基幹系システムのデータを分析対象としていた。しかし、2011年のMcKinsey Global Instituteのレポート発表以降、スマートフォンやSNSなど基幹系システム以外のデータも分析対象とすることで、他社との差別化を図り、企業価値を向上させることができると考えられるようになった。基幹系システムのデータも含めた、企業内外に存在するあらゆるデータの総称がビッグデータであり、ビッグデータという言葉が注目を集めることとなった。

　他にもビッグデータが注目されるようになった理由は2つある。1つは総務省の情報通信白書で先進企業におけるビッグデータの利用実態が公開され、ビッグデータ活用の有効性が各企業に認識されたこと。もう1つは、CPUコア数増加による性能向上やネットワーク帯域拡大など、ビッグデータ活用に必要な技術が革新的に進歩したことである。

### ビッグデータの課題

　これまでビッグデータが注目されている背景について述べてきたが、2015年時点でもビッグデータの活用が進んでいるとは言えない。その理由は、「投資対効果」「プライバシー・機密情報の取り扱い」「データの精度」「技術者不足」の4つの課題があるためである。

　ビッグデータ活用のためには多額のIT投資が必要になるが、投資対効果の算出が難しく、経営層の承認を得るのが難しい。また、ビッグデータ活用を始めたとしても、プライバシー、機密情報への抵触を避けるために、データの活用が進まないことや、収集したデータの精度が悪く、意図していた活用ができないこともある。

　さらに、RDBMS以外のデータベース管理システムであるNoSQLや、大量のデータを複数のサーバーで分散管理して並列に処理するインメモリデータグリッドなど新しい技術が次々と出てきても、対応できる技術者は、育成が進まず慢性的に不足している。

　技術者の不足については、HadoopなどでのSQLインタフェースへの対応のように、多くの人が利用できるような機能拡張が行われている。しかし、利用者視点ではDWH向けRDBMSとHadoopのどちらを採用すべきか分かり難く、適用しても期待した性能要件を満たせない可能性がある。そうならないためには、各々のアーキテクチャや特性を理解した上で、適用を検討する必要がある。

## DWH向けRDBMSとHadoopの技術動向と問題提起

### DWH向けRDBMSの技術動向

　DWHとは1990年代初頭にビル・インモン（William H. Inmon）によって提唱された、企業ビジネスの意思決定を支援するデータの格納庫である。DWHは基幹システムのデータを加工して取り込み、目的別にデータマートを作成していた。また、DWHの実装にはOracle、SQL Server、DB2などの汎用RDBMSが採用されていた。しかし、汎用RDBMSでは、大量データを扱うには大量のコンピュータリソースを搭載した高価なハードウェアが必要となるため、限られた企業のみが大量データを活用していた。

　2000年代に入ってから、専用のハードウェアにより大幅な性能向上を実現させたDWH専用のアプライアンスが登場したが、SNSなどで大量に発生するログデータを処理するために、さらなる性能向上のニーズが高まった。

　これに対応するため、マスターノードの役割を固定しない超並列処理と、処理に必要な列だけを扱う列（カラム）指向のアーキテクチャを実装した新型DWH向けRDBMSが登場した。超並列処理はデータ量に応じたスケールアウトを可能とし、列指向技術はデータ圧縮技術と組み合わせることで、I/O処理の削減とメモリの有効活用により性能向上を実現している。

　これらのニーズに応えるために、日本ユニシスでも2013年から、超並列処理と列指向技術を採用したHewlett Packard EnterpriseのHP Vertica Analytics Platform（以下、Vertica）を新型DWH向けRDBMS製品として販売開始した。

### Hadoopの技術動向

　Hadoopは、並列処理を行うためのオープンソースミドルウェアである。並列処理は、ひとつの計算処理を分割し、複数のサーバーで並列に処理することにより処理時間を短縮する技術である。2003年と2004年にGoogleが発表した並列処理に関する二つの論文「The Google File System」、「MapReduce: Simplified Data Processing on Large Clusters」を参考に、Doug Cuttingを中心とする少数のメンバーにより開発が始まり、Apacheコミュニティのプロジェクトとして、現在も多数の開発者により日々開発が進められている。

　Hadoopは「HDFS（Hadoop Distributed File System）」と「MapReduce」の二つの主要技術から構成される。HDFSは、大容量のファイルを複数のサーバーに分割して格納する分散ファイルシステムである。MapReduceは、並列処理を行うJavaのフレームワーク／実行エンジンである。これらの技術により、大容量ファイル処理の時間短縮を実現している。

　並列処理の技術は、スーパーコンピュータで採用されている標準規格のMPI（Message Passing Interface）などHadoop以外にも存在する。それらとHadoopが大きく異なる点は耐障害性がフレームワークとして備わっている点である。Hadoopは安価なコモディティサーバーで動かすことを想定して開発されており、ハードウェア故障が発生することを前提としたアーキテクチャになっている。

　障害時にデータを欠損しないように、データの複製を別のサーバーに格納する。並列処理の途中でサーバーの障害が起きた場合には、行っていた処理を別のサーバーで自動的にやり直す仕組みが備わっている。そのため、利用者は障害発生時の考慮を意識することなく、ビジネスロジックの実装に専念できることから、ビッグデータの蓄積／処理基盤として注目を集めている。

　Hadoopの利用が進むにつれて、より多くの人が利用できるように周辺での機能拡張が行われている。それら機能拡張のコンポーネントはHadoopエコシステムと呼ばれる。例えば、Javaで実装する必要があったMapReduceを、SQLに似た言語で扱えるようにしたHiveがその代表である。さらに、標準SQLへの準拠度を上げ、対話型の処理を行うために、SQL on Hadoopも登場しており、その一つにDrillがある。Drillは、MapReduceを用いずにメモリで処理を行うことにより、処理速度を向上させている。

　また、技術力を持った先進的な企業でなくとも安心して使えるように商用ディストリビューションが提供されている。代表的なベンダはCloudera、Hortonworks、MapR Technologiesである。日本ユニシスでは2014年から、MapR TechnologiesのHadoopディストリビューション（以下、MapR）を販売開始した。MapRでは、I/O効率を向上させ、NFS（Network File System）でのアクセスを可能にしたHDFS互換の独自ファイルシステム（以下、MapR-FS）を採用している。

## SQL on Hadoopの適用範囲についての仮説と検証環境

### SQL on Hadoopの適用範囲についての仮説

　SQL on Hadoopの適用範囲について3つの性能の観点で仮説を検討する。1つ目は検索するデータを取り込む時間、2つ目はデータを検索する時間、3つ目は同時アクセスでの検索時間である。データ取り込みについては、HadoopはDWH向けRDBMSに比べ処理時間は短い。これはDWH向けRDBMSで、検索処理を速くするようなデータの並び替えやフォーマット変換などを取り込み時に行っているためである。検索処理については、データ取り込み時に独自フォーマットで格納しているDWH向けRDBMSの処理時間が一番短く、メモリで処理を行っているSQL on Hadoopは、MapReduceで処理を行うHiveに比べて処理時間が短い。同時アクセスについては、DWH向けRDBMSはリソースを一元的に管理することによって効率的にデータ共有を行っているため、Hadoopに比べて同時実行性能は高い。

　以上のことから、以下の仮説が考えられる。これらの仮説は次項で検証している。
（仮説1）Hadoopは、分析対象となる全てのデータの格納に向いている。
（仮説2）DWH向けRDBMSは、利用頻度が高いデータに絞り込んで格納する必要がある。
（仮説3）SQL on Hadoopは、対話型による試行錯誤的なデータ検索に向いている。
（仮説4）Hiveは、メモリサイズ以上の大量データを扱うバッチ処理に向いている。
（仮説5）DWH向けRDBMSは、多くの利用者が同時にアクセスするデータ検索に向いている。
したがって、SQL on Hadoopの適用範囲は図1のように、Hadoop（データレイク）内のデータに対する対話型の検索ツールとして有用であると考える。

[![](../_resources/9894efa027e52a2166ac61e7baec4bb2.jpg)](http://it.impressbm.co.jp/mwimgs/f/0/-/img_f07e4f012b93bb37a8cce0b2ff29031b47691.jpg)（図1）

[拡大画像表示](http://it.impressbm.co.jp/mwimgs/f/0/-/img_f07e4f012b93bb37a8cce0b2ff29031b47691.jpg)

### 検証項目

　前節で挙げた仮説を検証するため表1のように、データロードとデータ検索における単独実行と同時実行について検証する。データ検索の実行クエリは、データ分析で基本となるグループ集計とテーブル結合のクエリとする。

[![](../_resources/a3978ec366d63e23fe8b5a0feb99e431.jpg)](http://it.impressbm.co.jp/mwimgs/a/5/-/img_a5e5e2fa9ee5e959c56d23f22fc9a46062410.jpg)（表1）

[拡大画像表示](http://it.impressbm.co.jp/mwimgs/a/5/-/img_a5e5e2fa9ee5e959c56d23f22fc9a46062410.jpg)

 [【次ページ】検証内容・検証処理概要・検証テストは](http://it.impressbm.co.jp/articles/-/14096?page=2)

*ページ: 2*

### 前提条件

　検証における前提条件は、下記の通りである。
1）DWH向けRDBMSについては、日本ユニシスで採用しているVerticaを使用する。Verticaの前提条件は下記の通りとする。
ⅰ）スーパープロジェクション（列配置で最適化することにより高いデータ圧縮率を実現したデータセット）を作成する。
ⅱ）テーブルには、パーティションを設定する。
ⅲ）同時実行数はVerticaのデフォルト値（1サーバーのcore数と同じ）を使用する。
2）Hadoopには、日本ユニシスで採用しているMapRを使用する。MapRの前提条件は下記の通りとする。
ⅰ）Hadoopエコシステムの中で、SQLでデータを扱えるHiveとDrillを使用する。
ⅱ）Hiveテーブルには、パーティションを設定する。
ⅲ）Hiveテーブルは，以下二つの形式を作成する。
ア）デフォルト形式（テキストファイル形式）。
イ）列単位にデータ保存を行うParquet形式。
ⅳ）上記のHiveテーブルの形式に適用する圧縮アルゴリズムは、下記の通りとする。
ア）デフォルト形式の場合は、MapR-FS自動圧縮機能によりLZ4で圧縮される。
イ）Parquet形式の場合は、圧縮解凍処理が速く、I/O処理が高速となるsnappyを適用する（以下、P+s圧縮形式）。
ⅴ）DrillはHiveテーブルからデータを検索する。
3）データロードを行う前のデータ形式はcsv形式とする。
4）データロードを行う前のデータはMapR-FSに配置する。
5）データ検索の処理時間は、クエリを発行してから全結果がファイルに出力完了するまでとする。

### 検証環境の構成

　検証環境として、図2、表2のように、VMwareを用いた仮想サーバーを3台作成する。尚、仮想サーバーは物理サーバーと1対1となるように作成する。
　MapRとVerticaのインストールは、リソースの使用状況などOSに関する条件を同じにするため、同じ仮想サーバー3台に対して行う。
　ネットワークは、Verticaで推奨されているPrivate（ノード内通信用）とPublic（外部との通信用）の2系統とする。
　MapRとVerticaの設定値は、1TB程度のデータ量を処理できるように修正している。

[![](../_resources/9c4b2659b32892f595125f92fb341993.jpg)](http://it.impressbm.co.jp/mwimgs/4/c/-/img_4cbe68b65831f1e79170054e5f68242073346.jpg)（図2）

[拡大画像表示](http://it.impressbm.co.jp/mwimgs/4/c/-/img_4cbe68b65831f1e79170054e5f68242073346.jpg)

[![](../_resources/80a5e7fe989f74d2a2c0e8e80cc8d34a.jpg)](http://it.impressbm.co.jp/mwimgs/d/e/-/img_de692be204823e16b2084506cb9cfa8881935.jpg)（表2）

[拡大画像表示](http://it.impressbm.co.jp/mwimgs/d/e/-/img_de692be204823e16b2084506cb9cfa8881935.jpg)

### 検証処理概要

　前節で挙げた検証項目の処理概要は、図3の通りである。

[![](../_resources/39b3fe0014d4eec77d09a528e63cd7f2.jpg)](http://it.impressbm.co.jp/mwimgs/b/9/-/img_b9713c861f83bc671661ca86e645af0365206.jpg)（図3）

[拡大画像表示](http://it.impressbm.co.jp/mwimgs/b/9/-/img_b9713c861f83bc671661ca86e645af0365206.jpg)

　MapR（Hive）のデータロードでは、MapR-FSに格納されたデータをデフォルト形式のHiveテーブルにロードする（①）。P+s圧縮形式のHiveテーブルにデータロードを行う場合は、デフォルト形式のHiveテーブルに格納されたデータをロードする（②）。

　MapR（Hive、Drill）のデータ検索では、各形式のHiveテーブルからデータ検索を行い、出力結果を仮想サーバー1のローカルディスクに出力する（③）。
　Verticaのデータロードでは、NFSマウントしたMapR-FSに格納されたデータをVerticaのテーブルにロードする（④）。
　Verticaのデータ検索では、MapRと同様、出力結果を仮想サーバー1のローカルディスクに出力する（⑤）。

### 検証データ

　検証データのテーブル構造は、図4の通りである。結合するテーブルの件数、サイズの違いによる処理性能を確認するため、検証テーブルと結合するマスターテーブルを3つ用意している。また、同時実行の検証のみ、検証テーブルのサイズを0.33TBとした。

[![](../_resources/cbf9cb6d761224cd6b5bd3a35970b302.jpg)](http://it.impressbm.co.jp/mwimgs/d/8/-/img_d8fd19dde1e700ab12ee51ead93412ad68938.jpg)（図4）

[拡大画像表示](http://it.impressbm.co.jp/mwimgs/d/8/-/img_d8fd19dde1e700ab12ee51ead93412ad68938.jpg)

　　　　　　　　　　　　　　　　　　　　　　　（後編につづく）

### 参考文献

[1] James Manyika、Michael Chui、Brad Brown、Jacques Bughin、Richard Dobbs、Charles Roxburgh、Angela Hung Byers、「Big data: The next frontier for innovation, competition, and productivity」、McKinsey Global Institute 、2011年5月

[2] Sanjay Ghemawat、 Howard Gobioff、 Shun-Tak Leung、「The Google File System」、2003年

[3] Jeffrey Dean and Sanjay Ghemawat、「MapReduce: Simplified Data Processing on Large Clusters」、2004年

[4] Michael Manoochehri、小林啓倫、「ビッグデータテクノロジー完全ガイド」、2014年11月
[5] 山崎慎一、「企業情報システムとデータの活用範囲の拡大」、ユニシス技報、日本ユニシス、Vol.31 No.4 通巻111号、2012年3月
[6] 羽生貴史、「データベース技術の動向」、ユニシス技報、日本ユニシス、Vol.29 No.2、通巻101号、2009年3月
[7] 総務省、「平成24年版情報通信白書」、2012年6月
[8] 総務省、「平成25年版情報通信白書」、2013年7月
[9] 鈴木良介、「ビッグデータビジネスの時代」、株式会社翔泳社、2012年3月

[10] Edward Capriolo、Dean Wampler、Jason Rutherglen、玉川竜司訳、「プログラミングHive」、オライリー･ジャパン（発行元）、2013年6月

[11] W.H.Inmon、藤本康秀、小畑喜一、「初めてのデータウェアハウス構築」、1995年12月
[12] 日本ユニシス、「データ統合・分析共通PaaS」
http://www.unisys.co.jp/solution/biz/bigdata/solution/paas.html
清酒　文人
小原　未生
日本ユニシス